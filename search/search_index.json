{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Profil Nama : Galih Restu Baihaqi Nim : 190411100177 Kelas : Web Mining Prodi : Teknik Informatika Universitas Trunojoyo Madura","title":"Home"},{"location":"Code-Evaluasi-Klasifikasi/","text":"Code menggunakan data https://drive.google.com/file/d/1XvI1idAHVFl94eQo2tt9phXNUPNEGZx7/view?usp=sharing dengan inialisasi Laki Laki = 1 dan Perempuan = 2 (dalam data) Mengimport Library import pandas as pd import numpy as np from sklearn import svm from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix Diatas adalah Library yang di import dalam kode ini. Memasukkan data data = ( 'data.csv' ) df2 = pd . read_csv ( data ) X = np . asarray ( df2 ) X_atribut = X [ 1 : 201 , 0 : 4 ] Y = X [ 1 : 201 , 4 ] X_train , X_test , y_train , y_test = train_test_split ( X_atribut , Y , test_size = 1 , random_state = 0 ) Kemudian data di masukkan dengan di inialisasikan menjadi variabel data, lalu di read menggunakan fungsi pada pandas pd.read_csv. karena data yang digunakan bertype csv, Lalu merubahnya menjadi bentuk array dengan mengguanakan numpy. selanjutnya memisahkan nilai atribut dengan nilai labelnya, lalu formatnya [mulaibaris:sampaibaris, awalkolom:sampaikolom] lalu memisahkan nilai nilai menjadi nilai training dan juga menjadi nilai testing dengan nilai yang didapat dari train_test_split(). Permodelan dan memprediksi model = svm . SVC () model . fit ( X_train , y_train ) y_predict = model . predict ( X_test ) print ( \"prediksi:\" , y_predict ) print ( \"testing:\" , y_test ) menggunakan algoritma SVM, lalu nilai dari masing masing di fit kedalam variabel masing masing dengan nilai hasil dari pediksinya. kemudian print hasilnya Melihat hasil performa matrix = confusion_matrix ( y_test , y_predict ) tn , fp , fn , tp = matrix . ravel () print ( \"True Negatif:\" , tn ) print ( \"False Positif:\" , fp ) print ( \"False Negatif:\" , fn ) print ( \"True Positif:\" , tp ) kemudian melakukan perhitungan pada nilai y_test dan y_predictnya. dan membagikan nilai nilai kepada variabel tn, fp, fn, tp. sesuai dengan Confusion m atrix, Mencoba rumus akurasi = ( tp + tn ) / ( tp + fp + tn + fn ) errorrate = ( fp + fn ) / ( tp + fp + tn + fn ) print ( \"Nilai Akurasi =\" , akurasi ,) print ( \"Nilai Error Rate =\" , errorrate ) Kemudian masukkan rumus yang sama pada algoritma klasifikasi, dan akan mendapatkan hasil sesuai perhitungan.","title":"Code"},{"location":"Code-Text-Preprocesing/","text":"Case Folding Menjadikan Huruf Kecil kalimat = \"Dibawah ini adalah kabupaten yang ada di Madura, yaitu Sumenep, Pamekasan, Sampang dan Bangkalan\" lower_case = kalimat . lower () print ( lower_case ) Output dibawah ini adalah kabupaten yang ada di madura, yaitu sumenep, pamekasan, sampang dan bangkalan Fungsi lower adalah fungsi untuk membuat tulisan Kapital pada huruf menjadi huruf kecil Menghilangkan Angka import re kalimat = \"Madura memiliki 4 kabupaten\" hasil = re . sub ( r \"\\d+\" , \"\" , kalimat ) print ( hasil ) Output Madura memiliki kabupaten Dengan mengimport re lalu menggunakan fungsi Sub dan ditambahkan \\d+ yang merupakan reguler expression Menghilangkan Tanda Baca import string kalimat = \"Ini &adalah [contoh] kalimat? {dengan} tanda. baca?!!\" hasil = kalimat . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) print ( hasil ) Output Ini adalah contoh kalimat dengan tanda baca menghilangkan tanda baca agar hasil output adalah berupa text saja. Str.maketrans adalah fungsi dari Python untuk melakukan pemetaan karakter terhadap tabel. lalu mengambil 2 parameter, lalu string.punctuatuion akan menghapus tanda baca. Menghilangkan Karakter Kosong kalimat = \" \\b ini kalimat contoh \\b \" hasil = kalimat . strip () print ( hasil ) Output ini kalimat contoh Tujuannya untuk menghilangkan karakter karakter yang kosong dengan menggunakan fungsi strip(), strip() adalah fungsi dari python yang digunakan untuk menghapus awal dan akhir dalam sebuah kalimat yang sama Tokenisasi Libarary NLTK import nltk from nltk.tokenize import word_tokenize kalimat = \"Iffa adalah anak yang berbakti kepada orang tuanya\" tokens = nltk . tokenize . word_tokenize ( kalimat ) print ( tokens ) Output ['Iffa', 'adalah', 'anak', 'yang', 'berbakti', 'kepada', 'orang', 'tuanya'] Library nltk di import, lalu melakukan tokenisasi untuk memisakan kata dalam sebuah kalimat. Word_tokenize adalah fungsi yang digunakan untuk mengembalikan token. tokenisasi dan case folding from nltk.tokenize import word_tokenize kalimat = \"Iffa adalah anak yang berbakti kepada orang tuanya.\" kalimat = kalimat . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () tokens = nltk . tokenize . word_tokenize ( kalimat ) print ( tokens ) Outputnya ['iffa', 'adalah', 'anak', 'yang', 'berbakti', 'kepada', 'orang', 'tuanya'] Dengan menggabungkan keduanya, maka nilai tiitk, Nilai huruf besar akan hilang pada hasil outputnya, pada code diatas alah penggabungan case folding dan tokenisasi Menghitung Frekuensi kemunculan kata from nltk.tokenize import word_tokenize from nltk.probability import FreqDist kalimat = \"Andi dan Iffa adalah anak yang berbakti kepada orang tuanya. Andi dan Iffa tinggal di Bangkalan\" kalimat = kalimat . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () tokens = nltk . tokenize . word_tokenize ( kalimat ) kemunculan = nltk . FreqDist ( tokens ) print ( kemunculan . most_common ()) Output [('andi', 2), ('dan', 2), ('iffa', 2), ('adalah', 1), ('anak', 1), ('yang', 1), ('berbakti', 1), ('kepada', 1), ('orang', 1), ('tuanya', 1), ('tinggal', 1), ('di', 1), ('bangkalan', 1)] Frekuensi kemunculan setiap kata akan di hitung dengan Library FreqDist. FreqDist adalah kelas dari python yang sudah tersedia dalam NLTK yang tujuannya untuk menghitung frekuensi kemunculan token. Melihat Frekuensi Secara Visual import matplotlib.pyplot as plt kemunculan . plot ( 30 , cumulative = False ) plt . show () Output Kemunculan adalah variabel pada kode sebelumnya, yang kemudian di plot (), dengan nilai maximal dari x yaitu 30 kata dan komulatif bernilai salah. .plot(x,y) Pemisahan kalimat per paragraf from nltk.tokenize import sent_tokenize kalimat = \"Andi dan Iffa adalah anak yang berbakti kepada orang tuanya. Andi dan Iffa tinggal di Bangkalan\" tokens = nltk . tokenize . sent_tokenize ( kalimat ) print ( tokens ) Output ['Andi dan Iffa adalah anak yang berbakti kepada orang tuanya.', 'Andi dan Iffa tinggal di Bangkalan'] sent_tokenize adalah kelas pada python yang digunakan untuk memisahkan kalimat berdasarkan tiap paragraf Filtering dan StopWord Menggunakan NLTK from nltk.tokenize import sent_tokenize , word_tokenize from nltk.corpus import stopwords kalimat = \"Andi dan Iffa adalah anak yang berbakti kepada orang tuanya. Andi dan Iffa tinggal di Bangkalan\" kalimat = kalimat . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () tokens = word_tokenize ( kalimat ) listStopword = set ( stopwords . words ( 'indonesian' )) removed = [] for t in tokens : if t not in listStopword : removed . append ( t ) print ( removed ) Output ['andi', 'iffa', 'anak', 'berbakti', 'orang', 'tuanya', 'andi', 'iffa', 'tinggal', 'bangkalan'] Menggunakan Stopwords pada NLTK tujuaanya adalah menghilangkan kata kata yang tidak penting, dengan mengguanakan stopwords.words bertipe indonesia, yang kemudian ditaruh dalam sebuah variabel listStopword. dan kemudian isi dari stopword tersebut di looping satu persatu, dicari apakah ada kata yang sama di sana, lalu jika tidak ada maka di masukkan ke dalam list removed. Menggunakan Sastrawi melihat isi stopword pada sastrawi from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factory = StopWordRemoverFactory () stopwords = factory . get_stop_words () print ( stopwords ) Ketika di print akan muncul beberapa kata stopword didalam sastrawi Memfilter menggunakan sastrawi from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () kalimat = \"Andi dan Iffa adalah anak yang berbakti kepada orang tuanya. Andi dan Iffa tinggal di Bangkalan\" kalimat = kalimat . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () stop = stopword . remove ( kalimat ) tokens = nltk . tokenize . word_tokenize ( stop ) print ( tokens ) Output ['andi', 'iffa', 'anak', 'berbakti', 'orang', 'tuanya', 'andi', 'iffa', 'tinggal', 'bangkalan'] Kata kata yang sudah ada di dalam sastrawi, kemudian di stopword menggunakan sastrawi, otomatis kata yang menjadi stopword akan hilang. Menambahkan stopword from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory , StopWordRemover , ArrayDictionary from nltk.tokenize import word_tokenize # ambil stopword bawaan stop_factory = StopWordRemoverFactory () . get_stop_words () more_stopword = [ 'kepada' , 'tinggal' ] kalimat = \"Andi dan Iffa adalah anak yang berbakti kepada orang tuanya. Andi dan Iffa tinggal di Bangkalan\" kalimat = kalimat . translate ( str . maketrans ( '' , '' , string . punctuation )) . lower () # menggabungkan stopword data = stop_factory + more_stopword dictionary = ArrayDictionary ( data ) str = StopWordRemover ( dictionary ) tokens = nltk . tokenize . word_tokenize ( str . remove ( kalimat )) print ( tokens ) Output ['andi', 'iffa', 'anak', 'berbakti', 'orang', 'tuanya', 'andi', 'iffa', 'di', 'bangkalan'] Stopword pada sastrawi akan berjalan lalu ditambah lagi dengan stopword tambahan dari user, seperti pada kata Kepada dan Tinggal. Stemming Menggunankan NLTK from nltk.stem import PorterStemmer ps = PorterStemmer () kata = [ \"program\" , \"programs\" , \"programer\" , \"programing\" , \"programers\" ] for k in kata : print ( k , \" : \" , ps . stem ( k )) Output program : program programs : program programer : program programing : program programers : program Stemming adalah proses mengubah kata menjadi kata dasar, dengan menggunakan kelas pada NLTK yaitu PorterStemmer() Menggunakan Sastrawi from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory () stemmer = factory . create_stemmer () kalimat = \"Andi dan Iffa adalah anak yang berbakti kepada orang tuanya. Andi dan Iffa tinggal di Bangkalan\" hasil = stemmer . stem ( kalimat ) print ( hasil ) Output andi dan iffa adalah anak yang bakti kepada orang tua andi dan iffa tinggal di bangkal Menggunakan sastrawi juga bisa menstem sebuah kata, Efeknya ketika menginputkan nama daerah, maka akan menjadi kata dasar juga seperti bangkalan menjadi bangkal","title":"Code"},{"location":"Crawling/","text":"Web Crawl Pengertian Meng-Crawl sebuah web adalah kegiatan untuk mengindekskan atau mengunduh konten yang ada pada internet kemudian disimpan kedalam suatu database mesin pencarian, hal ini biasa di sebut dengan spiders. Perlu mengimport scrapy yang merupakan ekstraksi data dari sebuah web Code import scrapy class ReviewSpider ( scrapy . Spider ): name = 'review' allowed_domains = [ 'goodnewsfromindonesia.id' ] start_urls = [ 'https://www.goodnewsfromindonesia.id/' ] def parse ( self , response ): data = response . css ( '.thumbnail-list--left' ) title = data . css ( '.thumbnail-list--title' ) cat = data . css ( '.thumbnail-list--category' ) time = data . css ( '.thumbnail-list--author' ) c = 0 for review in title : yield { 'title' : '' . join ( review . xpath ( './/text()' ) . extract ()), 'cat' : '' . join ( cat [ c ] . xpath ( './/text()' ) . extract ()), 'time' : '' . join ( time [ c ] . xpath ( './/text()' ) . extract ()) } c = c + 1 diatas adalah code untuk mengambil data dari sebuah web https://www.goodnewsfromindonesia.id/ start_url adalah variable yang menampung url yang akan di crawl datanya. kemudian membuat fungsi dengan parameter dirinya sendiri(self) dan response. kemudian inialisasikan css dengan cara ctrl+shift+i untuk meninspect halaman tersebut dan pilihlah class yang mewakili dari isi konten yang ada pada halaman tersebut. pada data = response.css('.thumbnail-list--left') menjelaskan bahwa class ini adalah class yang mewakili semua isi konten yang ada pada halaman tersebut lalu pada ,title = data.css('.thumbnail-list--title'), cat = data.css('.thumbnail-list--category'), time = data.css('.thumbnail-list--author') adalah class dari masing masing konten yang akan di ambil. title untuk mengambil title yang ada pada konten tersebut, lalu kategori adalah kategori dari kontennya dan juga waktu dari konten tersebut. ketika sudah membuat codingan seperti itu, jalankan dengan menggunakan konsep spider pada CMD di folder file tersebut berada Scrapy runspider crawling.py -o hasil.json ini merupakan perintah pada CMD dengan crawling.py adalah nama file yang berisi kodingan tersebut dan hasil.json adalah file yang akan dihasilkan dari hasil crawl tersebut. kemudian akan terdapat file dengan nama hasil.json, lalu agar cara membaca hasilnya dengan mudah atau lebih relatif mudah di pahami, gunakan kodingan berikut : import json fjson = open ( \"hasil.json\" ) data = json . loads ( fjson . read ()) name = \"Kategori\" penulis = \"Penulis\" for i in range ( len ( data )): print ( f \" { name } : { data [ i ][ 'cat' ] } \" , \" \\n \" ) print ( f \" { data [ i ][ 'title' ] } \" , \" \\n \" ) print ( f \" { penulis } : { data [ i ][ 'time' ] } \" , \" \\n\\n \" ) jadi kode tersebut gunanya untuk membaca hasil dari crawl yang merupakan format Json, sehingga mudah atau lazim untuk di lihat.","title":"Crawling"},{"location":"Evaluasi-Klasifikasi/","text":"Evaluasi Klasifikasi K-Fold Cross Validation K-Fold Cross Validation adalah suatu metode dalam mengevaluasi klasifikasi, tujuannya untuk memperoleh hasil yang lebih maksimal, caranya dengan menggunakan percobaan validasi sebanyak K kali untuk satu model dengan satu parameter yang sama. K-Fold Cross Validation juga bisa di katakan pengembangan dari model split validation, karena dalam melakukan validasi yang di ukur adalah training erornya dengan menggunakan test data atau data uji. Validasi ini muncul karena adanya sample yang tidak akurat ketika dilakukan dengan cara acak. Validasi ini mampu bekerja dengan cepat dengan cara melakukan pengambilan sampel yang lebih mempunyai struktur. gambar diatas adalah percobaan validasi ini sebanyak 5 kali tahapan. Percobaan perama , yaitu dengan cara menjadikan partisi pertama dijadikan sebagai data testing dan partisi partisi yang lainnya digunakan menjadi data training Percobaan Kedua , dengan menjadikan data kedua menjadi data testing dan kemudian pada data - data lainnya di ubah menjadi data training Percobaan ke 3 , Sama halnya dengan percobaan - percobaan sebelumnya, maka di percobaan ketiga adalah dengan membuat data di partisi ketiga menjadi data testing dan di partisi kedua menjadi data training, begitu terus sampai dengan banyak tahapan. Pengukuran Performa Klasifikasi Confusion Matrix Dalam pengukuran performa yang umum digunakan dalam evaluasi klasifikasi yaitu menggunakan Confusion Matrix. Gambar diatas adalah contoh Confusion matrix dalam kelas biner. jadi terdapat 2 nilai saja, yaitu 0 dan 1/true dan false / ya dan tidak. pada gambar di atas terdapat 2 kelas yaitu kelas aktual dan kelas prediksi. penjelasan soal gambar yaitu, ketika sama sama berniilai positif, maka akan menghasilkan TP (True Positif), lalu ketika sama sama bernilai negatif maka akan menghasilkan nilai TN(True Negatif). Ketika pada nilai aktualnya negatif dan nilai prediksinya positif, maka nilainya ialah FP(False Positif), dan ketika nilai aktualnya positif dan nilai prediksinya negatif maka menghasilkan nilai FN(False Negatif). Performa Algoritma Klasifikasi 1. Akurasi \\[ \\frac{TP + TN}{TP + FP + TN + FN} \\] 2. Error Rate \\[ \\frac{FP + FN}{TP + FP+TN+FN} \\] 3. Recall/Sensitifity \\[ \\frac{TP}{TP + FN} \\] 4.Spesificity \\[ \\frac{TN}{FP + TN} \\] 5. Precision \\[ \\frac{TP}{TP + FP} \\] 6. F-Force / Rata - Rata Harmonik \\[ \\frac{2*Precision*recall}{precision + recall} \\]","title":"Teori"},{"location":"Text-Preprocesing/","text":"Text Preprocesing Text Preprocesing adalah salah satu tahapan yang sangat penting yang digunakan untuk data ketika melakukan proses mining/ menambang. data - data yang ada dalam sebuah proses mining tidak selamanya ideal atau dalam kondisi pas untuk di proses. Beberapa masalah pasti ada dalam sebuah kata tersebut yang dapat mengganggu jalannya sebuah proses mining itu sendiri, seperti missing value, outliers, data redundant dan juga beberapa format data yang tidak sesuai dengan sistem yang dibuat. Preprocesing adalah tahapan untuk menghilangkan permasalahan yang dapat mengganggu hasil dari proses data. Preprocesing yang biasanya dilakukan untuk sebuah kalimat atau kata umumnya terdapat beberapa proses yaitu, Case Folding, Filtering, Stopword, Stemming, Tokenization. Beberapa tujuan dari Preprocesing adalah sebagai beriikut 1. Data Cleaning Beberapa data pasti mempunyai missing value ataupun istilahnya ialah noise data, yang disebabkan oleh pengumpulan data yang tidak benar dan menyebabkan hasilnya tidak relevan dan juga hilang, metode cleaning adalah salah satu cara untuk mengatasi masalah tersebut, seperti masalah pada missing value. Dengan menggunakan metode data cleaning kita bisa mengabaikan tupel. tupel adalah kumpulan data yang hilang, cara ini dapat dirasa ketika kita memiliki data yang besar dan memiliki missing value dengan tupel yang sama. Namun ketika datanya tidak terlalu besar, maka pendekatan pendekatan lain bisa dilakukan dengan adanya komputasi pada missing value tersebut. beberapa cara untuk data cleaning adalah clustering,regresi dan binning 2. Transformasi data Ketika missing value dan noise data sudah teratasi, preprocesing data berlanjut ke transformasi data. pada tahap Tranformasi data, datanya akan diubah menjadi sesuatu yang sesuai dengan metode analisis. beberapa prosesya seperti : Normalization : Proses membuat nilai skala pada data dalam sebuah range yang sudah ditentukan sebelumnya. Attributte Selection : Adalah proses membuat data baru dengan sebuah atribut sehingga kumpulan data dapat diatur dan mampu menganalisis data tersembunyi. Discretization : Adalah data yang diproses dengan cara ditransformasi dengan merubah nilai mentah atributnya dengan interval dalam numerik 3. Mengurangi data Dalam memilah data, baik data yang kecil ataupun data besar sekalipun itu memerlukan waktu yang lumayan lama, bisa menggunakan cara manual maupun cara otomatis, oleh karenanya, perlu pengurangan data agar data yang masuk semakin efisien dan mengurangi waktu proses serta penyimpanan. dalam proses pengurangan data ada beberapa langkah yaitu : Data Cube Aggregation : adalah array multidimensi yang didapat dari data organization Attribute Subset Selection : memilih atribut yang paling relevan dan sisanya akan dibuang Numerosity Reduction : merubah data yang asli menjadi representasi data yang lebih kecil Dimensionality Reduction : mereduksi data dengan mengurangi ukuran datanya.","title":"Teori"},{"location":"code_python_K-means/","text":"Data yang dipakai https://drive.google.com/file/d/142GqlYUo6-J16OPmLH1DWdhSZhSWXyOk/view?usp=sharing Berikut adalah Code Python untuk Melakukan Clustering K-Means dengan bantuan metode Elbow Mengimport Libary import numpy as np import matplotlib.pyplot as plt import pandas as pd Mengimport Library yang ada pada python, yaitu Numpy,matplotlib.pyplot, pandas lalu KMeans. Memasukkan Data dataset = pd . read_csv ( 'data.csv' ) X = dataset . iloc [:, [ 3 , 4 ]] . values dataset . dropna () pandas yang sudah di inialisasikan kedalam pd, kemudian menggunakan fungsi untuk membaca file berupa CSV read_csv('namafile'). Lalu dilakukan slice untuk kolom ke 3 dan ke 4, karena yang digunakan adalah kolom pendapatan dan rating pengeluaran. Metode Elbow from sklearn.cluster import KMeans wcss = [] for i in range ( 1 , 11 ): kmeans = KMeans ( n_clusters = i , init = 'k-means++' , random_state = 42 ) kmeans . fit ( X ) wcss . append ( kmeans . inertia_ ) plt . plot ( range ( 1 , 11 ), wcss ) plt . title ( 'K-Means Metode Elbow' ) plt . xlabel ( 'Total Clusters' ) plt . ylabel ( 'WCSS' ) plt . show () Library K-Means di import. Melakukan Looping 10 kali dengan membuat range 1,11 , yaitu dimulai dari satu dengan 10 kali looping. Melakukan algoritma Kmeans dengan memanggil KMeans yang di import dari library. Lalu data X diolah dengan dengan menggunakan kmeans.fit(), lalu melakukan perintah perhitungan dan juga di appendkan / dimasukkan ke dalam list wwcs. dengan nama K-means Metode Elbow, lalu pada sumbu x \"Total Cluster\" dan sumbu y \"WWCS\", kemudian Show. Data dalam K-Means kmeans = KMeans ( n_clusters = 5 , init = 'k-means++' , random_state = 42 ) y_kmeans = kmeans . fit_predict ( X ) print ( kmeans ) print ( y_kmeans ) Menjalankan perintah dengan jumlah cluster sebanyak 5, lalu melakukan prediksi dengan cara acak untuk subjek y_kmeans pada variabel X lalu akan mendapat hasil sebagai berikut KMeans(n_clusters=5, random_state=42) [3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 0 3 1 3 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 4 2 1 2 4 2 4 2 1 2 4 2 4 2 4 2 4 2 1 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4 2] Membuat hasil Visual plt . scatter ( X [ y_kmeans == 0 , 0 ], X [ y_kmeans == 0 , 1 ], s = 100 , c = 'red' , label = 'Cluster 1' ) plt . scatter ( X [ y_kmeans == 1 , 0 ], X [ y_kmeans == 1 , 1 ], s = 100 , c = 'blue' , label = 'Cluster 2' ) plt . scatter ( X [ y_kmeans == 2 , 0 ], X [ y_kmeans == 2 , 1 ], s = 100 , c = 'green' , label = 'Cluster 3' ) plt . scatter ( X [ y_kmeans == 3 , 0 ], X [ y_kmeans == 3 , 1 ], s = 100 , c = 'cyan' , label = 'Cluster 4' ) plt . scatter ( X [ y_kmeans == 4 , 0 ], X [ y_kmeans == 4 , 1 ], s = 100 , c = 'magenta' , label = 'Cluster 5' ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], s = 300 , c = 'yellow' , label = 'Centroids' ) plt . title ( 'Clusters Pelanggan' ) plt . xlabel ( 'Pendapatan Dalam tahunan (jutaan)' ) plt . ylabel ( 'Rate pengeluaran (1-100)' ) plt . legend () plt . show () Pada sumbu X, X[y_kmeans == 0, 0] maksudnya data berasal dari objek X, lalu untuk y_kmeans == 0 adalah perwakilan baris X yang dipilih. yaitu masuk ke cluster 1. lalu untuk sumbu y, X[y_kmeans == 0, 1] karena untuk kolom ke2, pada python ditulis satu karena dimulai dari 0.","title":"Code"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY Support Author \u00b6 Amazon wish list","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#support_author","text":"Amazon wish list","title":"Support Author"},{"location":"material-for-mkdocs/","text":"Material for MkDocs \u00b6 MkDocs \u00b6 mkdocs/mkdocs: Project documentation with Markdown - GitHub Material for MkDocs \u00b6 squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs","text":"","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#mkdocs","text":"mkdocs/mkdocs: Project documentation with Markdown - GitHub","title":"MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs_1","text":"squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"modelling/","text":"Tentang K-Means K-Means Clustering merupakan suatu metode yang digunankan untuk sebuah penganalisaan data yang proses pemodelannya dilakukan tanpa supervisi / unsupervised. K-Means juga merupakan sebuah metode yang melakukan pengelompokan data menggunakan sistem partisi. K-Means merupakan metode dalam data clustering non-hierarchical atau juga disebut Partitional Clustering. Umumnya dalam pengelompokan data yang sering digunakan adalah hierarchical dan Non-hierarchical. K-Means biasanya mengelompokkan data yang ada kedalam suatu kelompok. Data yang ada di dalam satu kelompok itu harus sama dalam sebuah karakteristiknya dan berbeda dengan data yang ada pada kelompok lainnya. Algoritma K-Means Clustering Menentukan jumlah cluster Seling data kedalam cluster dengan cara acak Menghitung rata-rata yang ada pada setiap cluster Seling masing - masing data ke rata rata terdekatnya Lakukan lagi dari step 3 Rumus untuk menghitung jarak antara data dengan centroid $$ D(a,b) = \\sqrt\\Sigma^n_{i=1}(b_{i} - a_{i})^2 $$ Karakteristik dari K-Means K-Means cepat dalam melakukan clustering Sensitif dalam centroid awal secara acak Memungkinkan cluster tidak mendapatkan anggota Hasilnya berubah - ubah, kadang akurat, namun terkadang juga sangat buruk K-Means sulit dalam mencapai global optimum","title":"Teori"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 \\[ P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha \\]","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"\\[ P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha \\]","title":"Example rendering"},{"location":"getting-started/docker/","text":"Start with Docker \u00b6 Public docker image \u00b6 Package mkdocs-material - GitHub peaceiris/mkdocs-material - Docker Hub docker-compose \u00b6 Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"Start with Docker"},{"location":"getting-started/docker/#start_with_docker","text":"","title":"Start with Docker"},{"location":"getting-started/docker/#public_docker_image","text":"Package mkdocs-material - GitHub peaceiris/mkdocs-material - Docker Hub","title":"Public docker image"},{"location":"getting-started/docker/#docker-compose","text":"Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"docker-compose"},{"location":"getting-started/download-boilerplate/","text":"Download boilerplate \u00b6 Git clone \u00b6 git clone https://github.com/peaceiris/mkdocs-material-boilerplate.git cd mkdocs-material-boilerplate Download zip \u00b6 wget 'https://github.com/peaceiris/mkdocs-material-boilerplate/archive/master.zip' unzip master.zip cd mkdocs-material-boilerplate-master \ud83d\udc49 Click me to download zip","title":"Download boilerplate"},{"location":"getting-started/download-boilerplate/#download_boilerplate","text":"","title":"Download boilerplate"},{"location":"getting-started/download-boilerplate/#git_clone","text":"git clone https://github.com/peaceiris/mkdocs-material-boilerplate.git cd mkdocs-material-boilerplate","title":"Git clone"},{"location":"getting-started/download-boilerplate/#download_zip","text":"wget 'https://github.com/peaceiris/mkdocs-material-boilerplate/archive/master.zip' unzip master.zip cd mkdocs-material-boilerplate-master \ud83d\udc49 Click me to download zip","title":"Download zip"},{"location":"getting-started/invoke/","text":"Serve and open with invoke \u00b6 invoke \u00b6 Invoke is a Python (2.7 and 3.4+) library for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. It draws inspiration from various sources (make/rake, Fabric 1.x, etc) to arrive at a powerful & clean feature set. pyinvoke/invoke: Pythonic task management & command execution. Serve and open \u00b6 Run mkdocs serve and open browser automatically. inv serve Serving on localhost:8000 # set IP and port inv serve --dev-addr 'localhost:5000' # set config file inv serve --config-file ./mkdocs-sample.yml Show all tasks \u00b6 $ inv --list Available tasks: serve Serve site and open browser Show task help. $ inv serve --help Usage: inv [ oke ] [ --core-opts ] serve [ --options ] [ other tasks here ... ] Docstring: Serve site and open browser Options: -c STRING, --config-file = STRING Provide a specific MkDocs config -d STRING, --dev-addr = STRING IP address and port to serve documentation locally ( default: localhost:8000 ) Tasks are defined by tasks.py","title":"Serve and open with invoke"},{"location":"getting-started/invoke/#serve_and_open_with_invoke","text":"","title":"Serve and open with invoke"},{"location":"getting-started/invoke/#invoke","text":"Invoke is a Python (2.7 and 3.4+) library for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. It draws inspiration from various sources (make/rake, Fabric 1.x, etc) to arrive at a powerful & clean feature set. pyinvoke/invoke: Pythonic task management & command execution.","title":"invoke"},{"location":"getting-started/invoke/#serve_and_open","text":"Run mkdocs serve and open browser automatically. inv serve Serving on localhost:8000 # set IP and port inv serve --dev-addr 'localhost:5000' # set config file inv serve --config-file ./mkdocs-sample.yml","title":"Serve and open"},{"location":"getting-started/invoke/#show_all_tasks","text":"$ inv --list Available tasks: serve Serve site and open browser Show task help. $ inv serve --help Usage: inv [ oke ] [ --core-opts ] serve [ --options ] [ other tasks here ... ] Docstring: Serve site and open browser Options: -c STRING, --config-file = STRING Provide a specific MkDocs config -d STRING, --dev-addr = STRING IP address and port to serve documentation locally ( default: localhost:8000 ) Tasks are defined by tasks.py","title":"Show all tasks"},{"location":"getting-started/pip/","text":"Start with pip (Anaconda, Miniconda) \u00b6 pip install -r requirements.txt pip install -r requirements-dev.txt inv command is also available.","title":"Start with pip (Anaconda, Miniconda)"},{"location":"getting-started/pip/#start_with_pip_anaconda_miniconda","text":"pip install -r requirements.txt pip install -r requirements-dev.txt inv command is also available.","title":"Start with pip (Anaconda, Miniconda)"},{"location":"getting-started/pipenv/","text":"Start with pipenv \u00b6 pipenv \u00b6 Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. pypa/pipenv: Python Development Workflow for Humans. Install all packages \u00b6 pipenv sync --dev # Installs all packages specified in Pipfile.lock. Run MkDocs \u00b6 pipenv shell # Spawns a shell within the virtualenv. mkdocs serve Or, run mkdocs with pipenv run pipenv run mkdocs serve pipenv run \u00b6 pipenv task are also defined by Pipfile pipenv run version # mkdocs --version pipenv run help # mkdocs --help pipenv run inv serve # inv serve pipenv run serve # mkdocs serve pipenv run build # mkdocs build pipenv run deploy # mkdocs gh-deploy","title":"Start with pipenv"},{"location":"getting-started/pipenv/#start_with_pipenv","text":"","title":"Start with pipenv"},{"location":"getting-started/pipenv/#pipenv","text":"Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. pypa/pipenv: Python Development Workflow for Humans.","title":"pipenv"},{"location":"getting-started/pipenv/#install_all_packages","text":"pipenv sync --dev # Installs all packages specified in Pipfile.lock.","title":"Install all packages"},{"location":"getting-started/pipenv/#run_mkdocs","text":"pipenv shell # Spawns a shell within the virtualenv. mkdocs serve Or, run mkdocs with pipenv run pipenv run mkdocs serve","title":"Run MkDocs"},{"location":"getting-started/pipenv/#pipenv_run","text":"pipenv task are also defined by Pipfile pipenv run version # mkdocs --version pipenv run help # mkdocs --help pipenv run inv serve # inv serve pipenv run serve # mkdocs serve pipenv run build # mkdocs build pipenv run deploy # mkdocs gh-deploy","title":"pipenv run"},{"location":"hosting-and-deployment/aws-amplify-console/","text":"Host on AWS Amplify Console \u00b6 AWS Amplify Console You can use Password protection each branch. Use the following build specification YML file. mkdocs-material-boilerplate/amplify.yml","title":"Host on AWS Amplify Console"},{"location":"hosting-and-deployment/aws-amplify-console/#host_on_aws_amplify_console","text":"AWS Amplify Console You can use Password protection each branch. Use the following build specification YML file. mkdocs-material-boilerplate/amplify.yml","title":"Host on AWS Amplify Console"},{"location":"hosting-and-deployment/combinations/","text":"Hosting and Deployment \u00b6 GitHub Pages and GitHub \u00b6 Host source code on GitHub. Build and deploy with: mkdocs gh-deploy GitHub Actions GitLab Pages and GitLab \u00b6 Host source code on GitLab. Build and deploy with GitLab CI/CD. Netlify \u00b6 Host source code on: GitHub GitLab BitBucket Build and deploy with Netlify. AWS Amplify Console \u00b6 Host source code on: GitHub GitLab BitBucket AWS CodeCommit Build and deploy with AWS Amplify Console.","title":"Hosting and Deployment"},{"location":"hosting-and-deployment/combinations/#hosting_and_deployment","text":"","title":"Hosting and Deployment"},{"location":"hosting-and-deployment/combinations/#github_pages_and_github","text":"Host source code on GitHub. Build and deploy with: mkdocs gh-deploy GitHub Actions","title":"GitHub Pages and GitHub"},{"location":"hosting-and-deployment/combinations/#gitlab_pages_and_gitlab","text":"Host source code on GitLab. Build and deploy with GitLab CI/CD.","title":"GitLab Pages and GitLab"},{"location":"hosting-and-deployment/combinations/#netlify","text":"Host source code on: GitHub GitLab BitBucket Build and deploy with Netlify.","title":"Netlify"},{"location":"hosting-and-deployment/combinations/#aws_amplify_console","text":"Host source code on: GitHub GitLab BitBucket AWS CodeCommit Build and deploy with AWS Amplify Console.","title":"AWS Amplify Console"},{"location":"hosting-and-deployment/github-pages/","text":"Host on GitHub Pages \u00b6 Demo site on GitHub Pages (build & deploy with GitHub Actions) Build and deploy with GitHub Actions \u00b6 peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details. Build and deploy with mkdocs gh-deploy \u00b6 pipenv \u00b6 pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"Host on GitHub Pages"},{"location":"hosting-and-deployment/github-pages/#host_on_github_pages","text":"Demo site on GitHub Pages (build & deploy with GitHub Actions)","title":"Host on GitHub Pages"},{"location":"hosting-and-deployment/github-pages/#build_and_deploy_with_github_actions","text":"peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details.","title":"Build and deploy with GitHub Actions"},{"location":"hosting-and-deployment/github-pages/#build_and_deploy_with_mkdocs_gh-deploy","text":"","title":"Build and deploy with mkdocs gh-deploy"},{"location":"hosting-and-deployment/github-pages/#pipenv","text":"pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"pipenv"},{"location":"hosting-and-deployment/gitlab-pages/","text":"Host on GitLab Pages \u00b6 See .gitlab-ci.yml","title":"Host on GitLab Pages"},{"location":"hosting-and-deployment/gitlab-pages/#host_on_gitlab_pages","text":"See .gitlab-ci.yml","title":"Host on GitLab Pages"},{"location":"hosting-and-deployment/netlify/","text":"Host on Netlify \u00b6 Demo site on Netlify (build & deploy with Netlify) Create GitHub repository and deploy to Netlify with the following button in 1 min.","title":"Host on Netlify"},{"location":"hosting-and-deployment/netlify/#host_on_netlify","text":"Demo site on Netlify (build & deploy with Netlify) Create GitHub repository and deploy to Netlify with the following button in 1 min.","title":"Host on Netlify"}]}